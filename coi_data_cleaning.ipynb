{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26497382-a9d9-4a78-a977-0b5ac2892fa6",
   "metadata": {
    "id": "26497382-a9d9-4a78-a977-0b5ac2892fa6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178cda18-7a2d-47f4-adcb-e583ea8dae4e",
   "metadata": {
    "id": "178cda18-7a2d-47f4-adcb-e583ea8dae4e"
   },
   "outputs": [],
   "source": [
    "path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e72cb-d84e-4f23-83dd-87e262a22b9d",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0b48d3-0ec4-40db-a6de-b0ee988a1d0c",
   "metadata": {
    "id": "3a0b48d3-0ec4-40db-a6de-b0ee988a1d0c"
   },
   "outputs": [],
   "source": [
    "coi = pd.read_csv(path+'data_raw/COI_raw.csv', dtype={'geoid':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2257f2e8-201d-4ea5-b29c-1430c15dae68",
   "metadata": {
    "id": "2257f2e8-201d-4ea5-b29c-1430c15dae68"
   },
   "outputs": [],
   "source": [
    "pop = pd.read_csv(path+'data_raw/DECENNIALPL2010.P1-Data.csv', skiprows=[1], dtype={'P001001': 'str', 'P001001ERR': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704fe5de-f990-43c5-b5cb-424a3b65abd7",
   "metadata": {
    "id": "704fe5de-f990-43c5-b5cb-424a3b65abd7"
   },
   "outputs": [],
   "source": [
    "cross_ref = pd.read_excel(path+'data_raw/grf15_lea_tract.xlsx', dtype={'LEAID': 'str', 'TRACT': 'str'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e06f30-1fb4-4cd0-b316-1651defdac21",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qYZuUhXudRGQ",
   "metadata": {
    "id": "qYZuUhXudRGQ"
   },
   "outputs": [],
   "source": [
    "# Split COI data by year (2010 or 2015)\n",
    "def coi_split_year(coi):\n",
    "\n",
    "    coi_2010 = coi[coi['year'] == 2010]\n",
    "    coi_2015 = coi[coi['year'] == 2015]\n",
    "\n",
    "    return coi_2010, coi_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1L5shK6Xh6LV",
   "metadata": {
    "id": "1L5shK6Xh6LV"
   },
   "outputs": [],
   "source": [
    "# Split data from the cross reference between LEAID and census TRACT to get keys for splitting\n",
    "# both COI and SEDA data\n",
    "def leaid_split(cross_ref, year):\n",
    "    leaid = cross_ref['LEAID'].unique()\n",
    "\n",
    "    # Split to get test and train\n",
    "    # Use year as random state to get different splits for each year\n",
    "    leaid_train, leaid_test = train_test_split(leaid, test_size=0.2, random_state=year)\n",
    "\n",
    "    # Write LEAIDs to csv for use with SEDA data\n",
    "    leaid_train_ser = pd.Series(leaid_train)\n",
    "    filename = path + 'data_inprocess/leaids_train_' + str(year) + '.csv'\n",
    "    leaid_train_ser.to_csv(filename)\n",
    "\n",
    "    leaid_test_ser = pd.Series(leaid_test)\n",
    "    filename = path + 'data_inprocess/leaids_test_' + str(year) + '.csv'\n",
    "    leaid_test_ser.to_csv(filename)\n",
    "\n",
    "    # Take training and test sets of cross_ref dataset by matching with LEAIDs\n",
    "    cross_ref_train = cross_ref[cross_ref['LEAID'].isin(leaid_train)]\n",
    "    cross_ref_test = cross_ref[cross_ref['LEAID'].isin(leaid_test)]\n",
    "\n",
    "    return cross_ref_train, cross_ref_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "xoT2rVTn4Fq0",
   "metadata": {
    "id": "xoT2rVTn4Fq0"
   },
   "outputs": [],
   "source": [
    "# Preprocess cross_ref to calculate percentage of tract land area per school district\n",
    "def preprocess_cross_ref(cross_ref):\n",
    "\n",
    "    # Groupby tract to get total tract land area\n",
    "    cross_ref['LANDAREA_TOT'] = cross_ref.groupby('TRACT')['LANDAREA'].transform('sum')\n",
    "\n",
    "    # Divide tract land area per district by total tract land area to get percent\n",
    "    cross_ref['LANDAREA_PERC'] = cross_ref['LANDAREA'] / cross_ref['LANDAREA_TOT']\n",
    "\n",
    "    return cross_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jJDajXjwkpck",
   "metadata": {
    "id": "jJDajXjwkpck"
   },
   "outputs": [],
   "source": [
    "# Process and join COI data with census population data to get total population \n",
    "# to use in weighting COI indicators\n",
    "def preprocess_coi(coi, pop):\n",
    "\n",
    "    # Strip '1400000US' from beginning of GEO_ID field\n",
    "    pop.loc[:, 'TRACT'] = pop['GEO_ID'].str.replace('1400000US', '')\n",
    "\n",
    "    # Remove the revision indicator string from the total population value\n",
    "    pop.loc[:, 'pop_total'] = pop['P001001'].str.replace(r'\\(r[0-9]+\\)', '', regex=True)\n",
    "\n",
    "    # Just take new geoid column with the population total\n",
    "    pop_tract = pop[['TRACT', 'pop_total']].copy()\n",
    "\n",
    "    # Set population total value data type to int\n",
    "    pop_tract.loc[:, 'pop_total'] = pop_tract['pop_total'].astype('int64')\n",
    "\n",
    "    # Rename COI columns to pop_child (as opposed to pop_total from the census) and geoid to TRACT for joining\n",
    "    coi = coi.rename(columns={'pop': 'pop_child', 'geoid': 'TRACT'})\n",
    "\n",
    "    # Merge COI and census population\n",
    "    coi = coi.merge(pop_tract, on='TRACT')\n",
    "\n",
    "    return coi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "jRKZorz-mn_P",
   "metadata": {
    "id": "jRKZorz-mn_P"
   },
   "outputs": [],
   "source": [
    "# Train-test split COI data\n",
    "def train_test_split_coi(cross_ref, coi, pop, year):\n",
    "\n",
    "    # Preprocess the cross_ref data to get percent land area of each tract in a\n",
    "    # given school district\n",
    "    cross_ref_proc = preprocess_cross_ref(cross_ref)\n",
    "\n",
    "    # Subsection the cross_ref data based on spliting by LEAID\n",
    "    cross_ref_train, cross_ref_test = leaid_split(cross_ref_proc, year)\n",
    "\n",
    "    # Preprocess the COI data\n",
    "    coi_proc = preprocess_coi(coi, pop)\n",
    "\n",
    "    # Subset COI data by inner-joining with cross_ref subsections\n",
    "    coi_dist_train = cross_ref_train.merge(coi_proc, on='TRACT')\n",
    "    coi_dist_test = cross_ref_test.merge(coi_proc, on='TRACT')\n",
    "\n",
    "    return coi_dist_train, coi_dist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "-3GJ5NnSW-g4",
   "metadata": {
    "id": "-3GJ5NnSW-g4"
   },
   "outputs": [],
   "source": [
    "# Process without splitting for various clustering methods\n",
    "def process_coi_all(cross_ref, coi, pop, write=False):\n",
    "\n",
    "    # Preprocess the cross_ref data to get percent land area of each tract in a\n",
    "    # given school district\n",
    "    cross_ref_proc = preprocess_cross_ref(cross_ref)\n",
    "\n",
    "    # Preprocess the COI data\n",
    "    coi_proc = preprocess_coi(coi, pop)\n",
    "\n",
    "    # Merge COI data with cross_ref\n",
    "    coi_dist_all = cross_ref_proc.merge(coi_proc, on='TRACT')\n",
    "    \n",
    "    # Identify the numeric indicator columns\n",
    "    ind_column_names = ['ED_APENR', 'ED_ATTAIN', 'ED_COLLEGE', 'ED_ECENROL', 'ED_HSGRAD', 'ED_MATH', \n",
    "                        'ED_READING','ED_SCHPOV', 'ED_TEACHXP', 'ED_PRXECE', 'ED_PRXHQECE', \n",
    "                        'HE_FOOD', 'HE_GREEN', 'HE_HEAT', 'HE_HLTHINS', 'HE_OZONE', 'HE_PM25',\n",
    "                        'HE_VACANCY', 'HE_WALK', 'HE_SUPRFND', 'HE_RSEI', 'SE_POVRATE', 'SE_PUBLIC', \n",
    "                        'SE_HOME', 'SE_OCC', 'SE_MHE', 'SE_EMPRAT', 'SE_JOBPROX', 'SE_SINGLE']\n",
    "\n",
    "    # Non-indicator columns\n",
    "    non_ind_column_names = ['LEAID', 'NAME_LEA15', 'TRACT', 'COUNT', 'LANDAREA_PERC', 'year', 'pop_child', 'pop_total']\n",
    "\n",
    "    # Process training data\n",
    "\n",
    "    # Get the numeric indicator columns\n",
    "    X = coi_dist_all[ind_column_names]\n",
    "\n",
    "    # Make a pipeline for processing\n",
    "    # Use median for imputer strategy because some of the variable distributions are highly skewed\n",
    "    pipe = Pipeline([('impute', SimpleImputer(strategy='median')), ('scale', StandardScaler())])\n",
    "\n",
    "    # Fit/transform just the numeric indicator columns\n",
    "    X_transformed = pipe.fit_transform(X)\n",
    "\n",
    "    # Reconstitute the dataframe with transformed data\n",
    "    X_trans_df = pd.DataFrame(X_transformed, columns=ind_column_names)\n",
    "\n",
    "    # Get non-indicator columns from training df\n",
    "    coi_cols = coi_dist_all[non_ind_column_names]\n",
    "\n",
    "    # Merge non-indicator and transformed columns\n",
    "    coi_dist_prep = coi_cols.merge(X_trans_df, left_index=True, right_index=True)\n",
    "\n",
    "    # Weight indicators by total population and land area\n",
    "    coi_dist_prep = weight_coi(coi_dist_prep, ind_column_names)\n",
    "\n",
    "    # Group COI data by school district\n",
    "    coi_grp_dist = group_coi(coi_dist_prep, ind_column_names)\n",
    "\n",
    "    # Write the data out to csv\n",
    "    if write:\n",
    "        filename = path + 'data_cleaned/coi_district_grouped.csv'\n",
    "        coi_grp_dist.to_csv(filename)\n",
    "\n",
    "    return coi_grp_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "syYDr5A1DlKA",
   "metadata": {
    "id": "syYDr5A1DlKA"
   },
   "outputs": [],
   "source": [
    "# Create population-weighted averages for COI indicators, scaled by the percent of tract land area\n",
    "# in the school district\n",
    "def weight_coi(coi_indicators, ind_column_names):\n",
    "\n",
    "    # Non-indicator column names to be merged back in after weighting\n",
    "    non_ind_cols = ['LEAID', 'NAME_LEA15', 'TRACT', 'year', 'pop_child', 'pop_total', 'pop_scaled']\n",
    "\n",
    "    # Scale the total population by the percentage of tract land area that is in a given \n",
    "    # school district\n",
    "    coi_indicators['pop_scaled'] = coi_indicators['pop_total'] * coi_indicators['LANDAREA_PERC']\n",
    "\n",
    "    # Weight the indicators by the scaled population from the census data\n",
    "    coi_weighted = coi_indicators[ind_column_names].multiply(coi_indicators['pop_scaled'], axis='index')\n",
    "\n",
    "    # Merge back in the LEA, tract, year, and population columns\n",
    "    coi_weighted = coi_indicators.loc[:, non_ind_cols].merge(coi_weighted, left_index=True, right_index=True)\n",
    "\n",
    "    return coi_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pQS5LbD7Nbua",
   "metadata": {
    "id": "pQS5LbD7Nbua"
   },
   "outputs": [],
   "source": [
    "# Group weighted indicators into school districts, then divide by total population to get a \n",
    "# weighted average\n",
    "def group_coi(coi_district, ind_column_names):\n",
    "\n",
    "    # Group by school district ID/name and year\n",
    "    coi_grouped = coi_district.groupby(['LEAID', 'NAME_LEA15', 'year']).sum()\n",
    "\n",
    "    # Divide the weighted indicators by the total population of the district to get the weighted average\n",
    "    coi_grouped.loc[:, ind_column_names] = coi_grouped.loc[:, ind_column_names].divide(coi_grouped['pop_scaled'], axis='index')\n",
    "\n",
    "    # Reset the multi-index\n",
    "    coi_grouped = coi_grouped.reset_index()\n",
    "\n",
    "    return coi_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nSabyV2PeyLe",
   "metadata": {
    "id": "nSabyV2PeyLe"
   },
   "outputs": [],
   "source": [
    "# Complete processing of train/test data\n",
    "def process_coi(coi, pop, cross_ref, year, write=False):\n",
    "\n",
    "    # Preprocess cross-ref, split cross-ref LEAIDs, preprocess COI, split COI/cross-ref into train/test\n",
    "    coi_dist_train, coi_dist_test = train_test_split_coi(cross_ref, coi, pop, year)\n",
    "\n",
    "    # Identify the numeric indicator columns\n",
    "    ind_column_names = ['ED_APENR', 'ED_ATTAIN', 'ED_COLLEGE', 'ED_ECENROL', 'ED_HSGRAD', 'ED_MATH', \n",
    "                        'ED_READING','ED_SCHPOV', 'ED_TEACHXP', 'ED_PRXECE', 'ED_PRXHQECE', \n",
    "                        'HE_FOOD', 'HE_GREEN', 'HE_HEAT', 'HE_HLTHINS', 'HE_OZONE', 'HE_PM25',\n",
    "                        'HE_VACANCY', 'HE_WALK', 'HE_SUPRFND', 'HE_RSEI', 'SE_POVRATE', 'SE_PUBLIC', \n",
    "                        'SE_HOME', 'SE_OCC', 'SE_MHE', 'SE_EMPRAT', 'SE_JOBPROX', 'SE_SINGLE']\n",
    "\n",
    "    # Non-indicator columns\n",
    "    non_ind_column_names = ['LEAID', 'NAME_LEA15', 'TRACT', 'COUNT', 'LANDAREA_PERC', 'year', 'pop_child', 'pop_total']\n",
    "\n",
    "    # Process training data\n",
    "\n",
    "    # Get the numeric indicator columns\n",
    "    X_train = coi_dist_train[ind_column_names]\n",
    "\n",
    "    # Make a pipeline for processing\n",
    "    # Use median for imputer strategy because some of the variable distributions are highly skewed\n",
    "    pipe = Pipeline([('impute', SimpleImputer(strategy='median')), ('scale', StandardScaler())])\n",
    "\n",
    "    # Fit/transform just the numeric indicator columns\n",
    "    X_transformed = pipe.fit_transform(X_train)\n",
    "\n",
    "    # Reconstitute the dataframe with transformed data\n",
    "    X_trans_df = pd.DataFrame(X_transformed, columns=ind_column_names)\n",
    "\n",
    "    # Get non-indicator columns from training df\n",
    "    coi_cols = coi_dist_train[non_ind_column_names]\n",
    "\n",
    "    # Merge non-indicator and transformed columns\n",
    "    coi_dist_train_prep = coi_cols.merge(X_trans_df, left_index=True, right_index=True)\n",
    "\n",
    "    # Weight indicators by total population and land area\n",
    "    coi_dist_train_prep = weight_coi(coi_dist_train_prep, ind_column_names)\n",
    "\n",
    "    # Group COI data by school district\n",
    "    coi_grp_dist_train = group_coi(coi_dist_train_prep, ind_column_names)\n",
    "\n",
    "    # Write the data out to csv\n",
    "    if write:\n",
    "        filename = path + 'data_cleaned/coi_district_grouped_train_' + str(year) + '.csv'\n",
    "        coi_grp_dist_train.to_csv(filename)\n",
    "\n",
    "    # Process test data\n",
    "\n",
    "    # Get the numeric indicator columns\n",
    "    X_test = coi_dist_test[ind_column_names]\n",
    "\n",
    "    # Just transform just the numeric indicator columns (use pipeline fitted above)\n",
    "    X_transformed = pipe.transform(X_test)\n",
    "\n",
    "    # Reconstitute the dataframe with transformed data\n",
    "    X_trans_df = pd.DataFrame(X_transformed, columns=ind_column_names)\n",
    "\n",
    "    # Get non-indicator columns from training df\n",
    "    coi_cols = coi_dist_test[non_ind_column_names]\n",
    "\n",
    "    # Merge non-indicator and transformed columns\n",
    "    coi_dist_test_prep = coi_cols.merge(X_trans_df, left_index=True, right_index=True)\n",
    "\n",
    "    # Weight indicators by total population and land area\n",
    "    coi_dist_test_prep = weight_coi(coi_dist_test_prep, ind_column_names)\n",
    "\n",
    "    # Group COI data by school district\n",
    "    coi_grp_dist_test = group_coi(coi_dist_test_prep, ind_column_names)\n",
    "\n",
    "    # Write the test data out to csv\n",
    "    if write:\n",
    "        filename = path + 'data_cleaned/coi_district_grouped_test_' + str(year) + '.csv'\n",
    "        coi_grp_dist_test.to_csv(filename)\n",
    "\n",
    "    return coi_grp_dist_train, coi_grp_dist_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c861f8c-10e8-4c27-b6aa-55fb979e5ba3",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "poXbUjusea4I",
   "metadata": {
    "id": "poXbUjusea4I"
   },
   "outputs": [],
   "source": [
    "# Split COI by year (2010, 2015)\n",
    "coi_2010, ooi_2015 = coi_split_year(coi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1UelPShCf0ji",
   "metadata": {
    "id": "1UelPShCf0ji"
   },
   "outputs": [],
   "source": [
    "# Create train/test data for 2010 and 2015\n",
    "coi_grp_dist_train_2010, coi_grp_dist_test_2010 = process_coi(coi_2010, pop, cross_ref, year=2010, write=True)\n",
    "coi_grp_dist_train_2015, coi_grp_dist_test_2015 = process_coi(ooi_2015, pop, cross_ref, year=2015, write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4b14eb3-78a0-4747-a110-5bcda396475d",
   "metadata": {
    "id": "b4b14eb3-78a0-4747-a110-5bcda396475d"
   },
   "outputs": [],
   "source": [
    "# Process COI without train/test split for DBSCAN clustering (requires all data)\n",
    "coi_all = process_coi_all(cross_ref, coi, pop, write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30222ca3-c53e-40d5-ab8a-8ae9714c7ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "neJhMQ84f2qO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
